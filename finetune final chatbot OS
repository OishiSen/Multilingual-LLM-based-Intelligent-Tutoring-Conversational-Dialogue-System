# -*- coding: utf-8 -*-
"""Fine Tune my Chatbot OS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wW32_dOyLPyh-fwtPqHLKEMflvaMDfgf
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# 
# # We have to check which Torch version for Xformers (2.3 -> 0.0.27)
# from torch import __version__; from packaging.version import Version as V
# xformers = "xformers==0.0.27" if V(__version__) < V("2.4.0") else "xformers"
# !pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "oishisen100/os_lora_chat_model",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Input:
{}

### Response:
{}"""

if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

# alpaca_prompt = You MUST copy from above!

inputs = tokenizer(
[
    alpaca_prompt.format(
        "What is the work-energy theorem?", # instruction
        "What is the work-energy theorem?", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

# Define chatbot function
def chatbot_interface():
    print("Chatbot is ready! Type 'exit' to stop.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break

        # Prepare input for the model
        formatted_input = alpaca_prompt.format(user_input, "")
        inputs = tokenizer([formatted_input], return_tensors="pt").to("cuda")

        # Generate response
        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)##temperature,beam decoding,end symbol,top p
        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

        # Display response
        print("Chatbot:", response.strip())

# Start chatbot interface
if __name__ == "__main__":
    chatbot_interface()

# Define chatbot function
def chatbot_interface():
    print("Chatbot is ready! Type 'exit' to stop.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break

        # Prepare input for the model
        formatted_input = alpaca_prompt.format(user_input, "")
        inputs = tokenizer([formatted_input], return_tensors="pt").to("cuda")

        # Generate response
        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)##temperature,beam decoding,end symbol,top p
        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

        # Display response
        print("Chatbot:", response.strip())

# Start chatbot interface
if __name__ == "__main__":
    chatbot_interface()